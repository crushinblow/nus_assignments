{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "Sy5h-3ABwUvs",
        "outputId": "a2dcffd9-0f01-4971-e39a-79ffcceaa0f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Reuters dataset...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training samples: 8982\n",
            "Test samples: 2246\n",
            "Number of categories: 46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/5\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 254ms/step - accuracy: 0.3542 - loss: 2.8876 - val_accuracy: 0.4780 - val_loss: 2.0819\n",
            "Epoch 2/5\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 238ms/step - accuracy: 0.5068 - loss: 2.0022 - val_accuracy: 0.5426 - val_loss: 1.7936\n",
            "Epoch 3/5\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 259ms/step - accuracy: 0.5684 - loss: 1.7102 - val_accuracy: 0.5899 - val_loss: 1.6280\n",
            "Epoch 4/5\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 249ms/step - accuracy: 0.6064 - loss: 1.5193 - val_accuracy: 0.6127 - val_loss: 1.5335\n",
            "Epoch 5/5\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 246ms/step - accuracy: 0.6354 - loss: 1.4261 - val_accuracy: 0.6422 - val_loss: 1.4792\n",
            "Test Accuracy: 0.6233\n"
          ]
        }
      ],
      "source": [
        "# Task 3\n",
        "#-----------------------------------------\n",
        "# Step 0: Import Required Libraries\n",
        "# -----------------------------------------\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 1: Load the Reuters Dataset\n",
        "# -----------------------------------------\n",
        "# The Reuters dataset contains short news articles classified into 46 categories.\n",
        "# Limit the vocabulary to the 10,000 most common words for simplicity.\n",
        "max_words = 10000  # Vocabulary size\n",
        "maxlen = 200       # Sequence length after padding (truncate/extend to 200 words)\n",
        "\n",
        "print(\"Loading Reuters dataset...\")\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words)\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}\")\n",
        "print(f\"Test samples: {len(x_test)}\")\n",
        "print(f\"Number of categories: {np.max(y_train) + 1}\")  # should be 46\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 2: Pad Sequences to the Same Length\n",
        "# -----------------------------------------\n",
        "# LSTM models require fixed-length input.\n",
        "# pad_sequences will ensure each news article has exactly 200 tokens.\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 3: One-Hot Encode the Labels\n",
        "# -----------------------------------------\n",
        "# Labels are integers (0–45). We need to convert them to one-hot vectors\n",
        "# for multi-class classification with 'categorical_crossentropy' loss.\n",
        "num_classes = np.max(y_train) + 1  # 46 classes\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 4: Build the LSTM Model\n",
        "# -----------------------------------------\n",
        "model = Sequential()\n",
        "\n",
        "# Layer 1: Embedding\n",
        "# - Turns word indices into dense vectors of length 128\n",
        "# - Input dimension: vocabulary size (max_words)\n",
        "# - Output dimension: embedding size (128)\n",
        "# - Input length: length of each padded sequence (maxlen)\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=maxlen))\n",
        "\n",
        "# Layer 2: LSTM Layer\n",
        "# - 64 units to capture sequential dependencies in text\n",
        "model.add(LSTM(64))\n",
        "\n",
        "# Layer 3: Output Layer\n",
        "# - Dense layer with softmax activation to output probabilities for each of 46 classes\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 5: Compile the Model\n",
        "# -----------------------------------------\n",
        "# Loss: categorical_crossentropy (multi-class classification)\n",
        "# Optimizer: Adam (efficient and adaptive)\n",
        "# Metric: accuracy (how often predictions match labels)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model architecture\n",
        "print(model.summary())\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 6: Train the Model\n",
        "# -----------------------------------------\n",
        "# - epochs: number of times the model sees the entire dataset\n",
        "# - batch_size: number of samples per gradient update\n",
        "# - validation_split: percentage of training data used for validation\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=64,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=1)\n",
        "\n",
        "# -----------------------------------------\n",
        "# Step 7: Evaluate the Model on Test Data\n",
        "# -----------------------------------------\n",
        "# This gives us a final accuracy score on unseen data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "avn9XErvwZdA"
      }
    }
  ]
}